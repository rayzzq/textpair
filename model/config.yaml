doc_encoder:
  hidden_size: 768
  num_heads: 4
  dropout: 0.3
  activation: gelu
  num_layers: 4

  sent_encoder:
    pretrain_model_name_or_path: cyclone/simcse-chinese-roberta-wwm-ext
    pooling_type: "last-avg"

task: single

sentence_classifier:
  seed: 42
  task: single
  pretrain_model_name_or_path: cyclone/simcse-chinese-roberta-wwm-ext
  data_dir: D:\NVIDIA\textpair\data
  output_dir: ./single

  train_args:
    lr: 1e-5
    gradient_clip_val: 0.5
    devices: 1
    weight_decay: 0.01
    batch_size: 4
    max_epochs: 4
    accelerator: gpu
    strategy: ddp
    save_steps: 1000




sentence_pair_classifier:
  seed: 42
  task: paired
  pretrain_model_name_or_path: cyclone/simcse-chinese-roberta-wwm-ext
  data_dir: D:\NVIDIA\textpair\data
  output_dir: ./paired

  train_args:
    lr: 1e-5
    gradient_clip_val: 0.5
    devices: 1
    weight_decay: 0.01
    batch_size: 4
    max_epochs: 4
    accelerator: gpu
    strategy: ddp
    val_check_interval: 0.5
    save_steps: 1000
